import numpy
import torch  

import pytorch_lightning as pl
from pytorch_lightning import loggers as pl_loggers

from pyrsd_emu import mlp
from pyrsd_emu import data_module
from typing import Any, Callable, Optional, List
import os

class PowerspectraDataset(torch.utils.data.Dataset):
    """
    Dataset class for powerspectra

    Data is a .npy array on disk

    Labels from external .npy array

    Parameters:
    -----------
    data_path:
        Path to powerspectra file
    transform:
        data augmentations to use
    """
    def __init__(
        self,
        data_path,
        transform,
        multipole_order=3,
    ):
        self.data_path  = data_path
        self.transforms = transform
        self.multipole_order = multipole_order
        
        self.param_min = np.array([1.8, 1.2, 0.6, 1., 0., 0., 0.55, 1.15, 0.5, 0., 0.], dtype=np.float32)
        self.param_max = np.array([3.0, 2.5, 1., 7., 0.25, 1., 2.35, 2.95, 0.9, 3., 18.], dtype=np.float32)
    
    def _open_file(self):
        self.hfile = h5py.File(self.data_path,'r')

    def __len__(self):
        with h5py.File(self.data_path, 'r') as hf:
            self.n_samples = hf['pk0_r'].shape[0]
                
        return self.n_samples

    def normalize_model_params(self, params):

        return (params - self.param_min)/(self.param_max - self.param_min)

    def __getitem__(self, idx: int):

        if not hasattr(self, 'hfile'):
            self._open_file()

        # concatenate desired multipoles along last dimension
        x = np.hstack(
            [self.hfile[f"pk{i*2}_r"][idx] for i in range(multipole_order)],
        )
        sig_x = np.hstack(
            [self.hfile[f"pk{i*2}_r_sig"][idx] for i in range(multipole_order)],
        )
        
        y = self.hfile['model_params'][idx]
        y = self.normalize_model_params(y) 
        # x = self.transforms(x) 

        return x, sig_x, y

class PowerspectraDataModule(pl.LightningDataModule):
    """
    Loads data from a single large hdf5 file,
    """
    
    def __init__(
        self,
        params: dict,
        train_transforms=None,
        val_transforms=None,
        test_transforms=None,
    ) -> None:
        
        super().__init__()
        
        self.params = params
        
        self.train_path = self.params.get("train_path", "./")
        self.val_path = self.params.get("val_path", "./") # Set val to same as train
        
        self.num_workers = self.params.get("num_workers", 1)
        self.batch_size = self.params.get("batch_size", 4)
        
        self.shuffle = self.params.get("shuffle", True)
        self.pin_memory = self.params.get("pin_memory", True)
        self.drop_last = self.params.get("drop_last", True) # drop last due to queue_size % batch_size == 0. assert in Moco_v2
                
    def _default_transforms(self) -> Callable:

   
        # transform = DecalsTransforms(self.params)
        transform = None
        
        return transform    
    
    def prepare_data(self) -> None:

        if not os.path.isfile(self.train_path):
            raise FileNotFoundError(
                """
                Your training datafile cannot be found
                """
            )
             
    def setup(self, stage: Optional[str] = None):
        
        # Assign train/val datasets for use in dataloaders     
        if stage == "fit" or stage is None:
    
            train_transforms = self._default_transforms() if self.train_transforms is None else self.train_transforms
            self.train_dataset = PowerspectraDataset(
                self.train_path,
                train_transforms,
            )
            

            # Val set not used for now
            val_transforms = self._default_transforms() if self.val_transforms is None else self.val_transforms
            self.val_dataset = PowerspectraDataset(
                self.val_path,
                val_transforms,
            )
        if stage == "predict" or stage is None:

            predict_transforms = CropTransform(self.params)
            # Predict over all training data
            self.predict_dataset = PowerspectraDataset(
                self.train_path,
                predict_transforms,
            )
                    
    def train_dataloader(self):
 
        loader = torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
        )
            
        return loader
    
    def val_dataloader(self):

        loader = torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
        )       
        
        return loader

    def predict_dataloader(self):
        
        loader = torch.utils.data.DataLoader(
            self.predict_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
        )       
        
        return loader
